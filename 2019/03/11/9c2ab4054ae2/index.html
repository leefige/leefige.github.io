<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="google-site-verification" content="dafl573mEfxvAtk2B1Ww7SzaBlhxcR6gucIGDaU1nPM"><meta name="msvalidate.01" content="60984751C8004A9EF9233E8C5EBF6884"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><script class="next-config" data-name="main" type="application/json">{"hostname":"leefige.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script><meta name="description" content="我们在训练神经网络模型时，最常用的就是梯度下降，这篇博客主要介绍下几种梯度下降的变种（mini-batch gradient descent和stochastic gradient descent），关于Batch gradient descent（批梯度下降，BGD）就不细说了（一次迭代训练所有样本），因为这个大家都很熟悉，通常接触梯队下降后用的都是这个。这里主要介绍Mini-batch"><meta property="og:type" content="article"><meta property="og:title" content="【转载】几种梯度下降方法和优化方法对比"><meta property="og:url" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/index.html"><meta property="og:site_name" content="leefige&#39;s blog"><meta property="og:description" content="我们在训练神经网络模型时，最常用的就是梯度下降，这篇博客主要介绍下几种梯度下降的变种（mini-batch gradient descent和stochastic gradient descent），关于Batch gradient descent（批梯度下降，BGD）就不细说了（一次迭代训练所有样本），因为这个大家都很熟悉，通常接触梯队下降后用的都是这个。这里主要介绍Mini-batch"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/1.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/3.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/4.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/2.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/2.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/8.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/9.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/10.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/11.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/12.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/13.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/14.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/15.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/16.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/17.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/18.jpg"><meta property="og:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/19.jpg"><meta property="article:published_time" content="2019-03-11T08:47:15.000Z"><meta property="article:modified_time" content="2024-12-18T15:38:17.971Z"><meta property="article:author" content="leefige"><meta property="article:tag" content="梯度下降"><meta property="article:tag" content="优化"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://leefige.github.io/2019/03/11/9c2ab4054ae2/1.jpg"><link rel="canonical" href="https://leefige.github.io/2019/03/11/9c2ab4054ae2/"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://leefige.github.io/2019/03/11/9c2ab4054ae2/","path":"2019/03/11/9c2ab4054ae2/","title":"【转载】几种梯度下降方法和优化方法对比"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>【转载】几种梯度下降方法和优化方法对比 | leefige's blog</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-135732873-1"></script><script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-135732873-1","only_pageview":false,"measure_protocol_api_secret":null}</script><script src="/js/third-party/analytics/google-analytics.js"></script><script src="/js/third-party/analytics/baidu-analytics.js"></script><script async src="https://hm.baidu.com/hm.js?257cf176e5b462c4f2bc5170104522a4"></script><link rel="dns-prefetch" href="https://blog-service-ruby.vercel.app"><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">leefige's blog</p><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="搜索" role="button"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#part-i-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94batch-gradient-descentmini-batch-gradient-descent-%E5%92%8C-stochastic-gradient-descent"><span class="nav-number">1.</span> <span class="nav-text">Part I: 梯度下降方法对比——Batch gradient descent、Mini-batch gradient descent 和 stochastic gradient descent</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80batch-gradient-descent"><span class="nav-number">1.1.</span> <span class="nav-text">一、Batch gradient descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8Cstochastic-gradient-descent"><span class="nav-number">1.2.</span> <span class="nav-text">二、stochastic gradient descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89mini-batch-gradient-descent"><span class="nav-number">1.3.</span> <span class="nav-text">三、Mini-batch gradient descent</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#part-ii-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95momentumnesterov-momentumadagradadadeltarmspropadam"><span class="nav-number">2.</span> <span class="nav-text">Part II: 深度学习中优化方法——momentum、Nesterov Momentum、AdaGrad、Adadelta、RMSprop、Adam</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87exponentially-weighted-average"><span class="nav-number">2.1.</span> <span class="nav-text">一、 指数加权平均（Exponentially weighted average）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%B8%A6%E5%81%8F%E5%B7%AE%E4%BF%AE%E6%AD%A3%E7%9A%84%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87bias-correction-in-exponentially-weighted-average"><span class="nav-number">2.2.</span> <span class="nav-text">二、带偏差修正的指数加权平均（bias correction in exponentially weighted average）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E5%8A%A8%E9%87%8Fmomentum"><span class="nav-number">2.3.</span> <span class="nav-text">三、动量（momentum）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9Bnesterov-momentum"><span class="nav-number">2.4.</span> <span class="nav-text">四、Nesterov Momentum</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94adagradadaptive-gradient"><span class="nav-number">2.5.</span> <span class="nav-text">五、AdaGrad（Adaptive Gradient）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%ADadadelta"><span class="nav-number">2.6.</span> <span class="nav-text">六、Adadelta</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83rmsproproot-mean-square-prop"><span class="nav-number">2.7.</span> <span class="nav-text">七、RMSprop（root mean square prop）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%ABadamadaptive-moment-estimation"><span class="nav-number">2.8.</span> <span class="nav-text">八、Adam（Adaptive Moment Estimation）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E8%AF%A5%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">2.9.</span> <span class="nav-text">八、该如何选择优化算法</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="leefige" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">leefige</p><div class="site-description" itemprop="description">曳尾于涂中</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">25</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">73</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/leefige" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;leefige" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i></a> </span><span class="links-of-author-item"><a href="mailto:templar_git@163.com" title="E-Mail → mailto:templar_git@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i></a></span></div></div></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://leefige.github.io/2019/03/11/9c2ab4054ae2/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="leefige"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="leefige's blog"><meta itemprop="description" content="曳尾于涂中"></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="【转载】几种梯度下降方法和优化方法对比 | leefige's blog"><meta itemprop="description" content=""></span><header class="post-header"><h1 class="post-title" itemprop="name headline">【转载】几种梯度下降方法和优化方法对比</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-03-11 16:47:15" itemprop="dateCreated datePublished" datetime="2019-03-11T16:47:15+08:00">2019-03-11</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2024-12-18 23:38:17" itemprop="dateModified" datetime="2024-12-18T23:38:17+08:00">2024-12-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a> </span></span><span id="/2019/03/11/9c2ab4054ae2/" class="post-meta-item leancloud_visitors" data-flag-title="【转载】几种梯度下降方法和优化方法对比" title="阅读次数"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Waline：</span> <a title="waline" href="/2019/03/11/9c2ab4054ae2/#waline" itemprop="discussionUrl"><span class="post-comments-count waline-comment-count" data-path="/2019/03/11/9c2ab4054ae2/" itemprop="commentCount"></span> </a></span><span class="post-meta-break"></span> <span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>14k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>17 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>我们在训练神经网络模型时，最常用的就是梯度下降，这篇博客主要介绍下几种梯度下降的变种（mini-batch gradient descent和stochastic gradient descent），关于Batch gradient descent（批梯度下降，BGD）就不细说了（一次迭代训练所有样本），因为这个大家都很熟悉，通常接触梯队下降后用的都是这个。这里主要介绍Mini-batch gradient descent和stochastic gradient descent（SGD）以及对比下Batch gradient descent、mini-batch gradient descent和stochastic gradient descent的效果。</p><span id="more"></span><h1 id="part-i-梯度下降方法对比batch-gradient-descentmini-batch-gradient-descent-和-stochastic-gradient-descent">Part I: 梯度下降方法对比——Batch gradient descent、Mini-batch gradient descent 和 stochastic gradient descent</h1><h2 id="一batch-gradient-descent">一、Batch gradient descent</h2><p>Batch gradient descent 就是一次迭代训练所有样本，就这样不停的迭代。整个算法的框架可以表示为：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations): <span class="comment">#num_iterations--迭代次数</span></span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure><p>Batch gradient descent的优点是理想状态下经过足够多的迭代后可以达到全局最优。但是缺点也很明显，就是如果你的数据集非常的大（现在很常见），根本没法全部塞到内存（显存）里，所以BGD对于小样本还行，大数据集就没法娱乐了。而且因为每次迭代都要计算全部的样本，所以对于大数据量会非常的慢。</p><h2 id="二stochastic-gradient-descent">二、stochastic gradient descent</h2><p>为了加快收敛速度，并且解决大数据量无法一次性塞入内存（显存）的问题，stochastic gradient descent（SGD）就被提出来了，SGD的思想是每次只训练一个样本去更新参数。具体的实现代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">permutation = <span class="built_in">list</span>(np.random.permutation(m))</span><br><span class="line">shuffled_X = X[:, permutation]</span><br><span class="line">shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>, m))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, m):  <span class="comment"># 每次训练一个样本</span></span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        AL,caches = forward_propagation(shuffled_X[:, j].reshape(-<span class="number">1</span>,<span class="number">1</span>), parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(AL, shuffled_Y[:, j].reshape(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(AL, shuffled_Y[:,j].reshape(<span class="number">1</span>,<span class="number">1</span>), caches)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br></pre></td></tr></table></figure><p>如果我们的数据集很大，比如几亿条数据，num_iterationsnum_iterations 基本上 设置1，2，（10以内的就足够了）就可以。但是SGD也有缺点，因为每次只用一个样本来更新参数，会导致不稳定性大些(可以看下图（图片来自ng deep learning 课），每次更新的方向，不想batch gradient descent那样每次都朝着最优点的方向逼近，会在最优点附近震荡）。因为每次训练的都是随机的一个样本，会导致导致梯度的方向不会像BGD那样朝着最优点。</p><p>注意：代码中的随机把数据打乱很重要，因为这个随机性相当于引入了“噪音”，正是因为这个噪音，使得SGD可能会避免陷入局部最优解中。</p><img data-src="/2019/03/11/9c2ab4054ae2/1.jpg" title="sgd"><p>下面来对比下SGD和BGD的代价函数随着迭代次数的变化图：</p><img data-src="/2019/03/11/9c2ab4054ae2/3.jpg" title="对比"><p>可以看到SGD的代价函数随着迭代次数是震荡式的下降的（因为每次用一个样本，有可能方向是背离最优点的）</p><h2 id="三mini-batch-gradient-descent">三、Mini-batch gradient descent</h2><p>mini-batch gradient descent 是batch gradient descent和stochastic gradient descent的折中方案，就是mini-batch gradient descent每次用一部分样本来更新参数，即 batch_sizebatch_size。因此，若batch_size=1batch_size=1 则变成了SGD，若batch_size=mbatch_size=m 则变成了batch gradient descent。</p><p>batch_sizebatch_size通常设置为2的幂次方，通常设置2，4，8，16，32，64，128，256，5122，4，8，16，32，64，128，256，512（很少设置大于512）。因为设置成2的幂次方，更有利于GPU加速。现在深度学习中，基本上都是用 mini-batch gradient descent，（在深度学习中，很多直接把mini-batch gradient descent（a.k.a stochastic mini-batch gradient descent）简称为SGD，所以当你看到深度学习中的SGD，一般指的就是mini-batch gradient descent）。下面用几张图来展示下mini-batch gradient descent的原理（图片来自ng deep learning 课）：</p><img data-src="/2019/03/11/9c2ab4054ae2/4.jpg" title="mini-batch"><p>下面直接给出mini-batch gradient descent的代码实现：</p><ol type="1"><li>首先要把训练集分成多个batch</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: random_mini_batches</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_mini_batches</span>(<span class="params">X, Y, mini_batch_size = <span class="number">64</span>, seed = <span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Creates a list of random minibatches from (X, Y)</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true &quot;label&quot; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(seed)            <span class="comment"># To make your &quot;random&quot; minibatches the same as ours</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                  <span class="comment"># number of training examples</span></span><br><span class="line">    mini_batches = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Shuffle (X, Y)</span></span><br><span class="line">    permutation = <span class="built_in">list</span>(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></span><br><span class="line">    num_complete_minibatches = m//mini_batch_size <span class="comment"># number of mini batches</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_complete_minibatches):</span><br><span class="line">        mini_batch_X = shuffled_X[:, k * mini_batch_size: (k + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: (k + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li>下面是在model中使用mini-batch gradient descent 进行更新参数</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">seed = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class="line">    seed = seed + <span class="number">1</span></span><br><span class="line">    minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line">    <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">        <span class="comment"># Select a minibatch</span></span><br><span class="line">        (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        AL, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(AL, minibatch_Y)</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(AL, minibatch_Y, caches)</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br></pre></td></tr></table></figure><p>下面来看mini-batch gradient descent 和 stochastic gradient descent 在下降时的对比图：</p><img data-src="/2019/03/11/9c2ab4054ae2/2.jpg" title="gd"><p>下面是mini-batch gradient descent的代价函数随着迭代次数的变化图：</p><img data-src="/2019/03/11/9c2ab4054ae2/2.jpg" title="gd"><p>从图中能够看出，mini-batch gradient descent 相对SGD在下降的时候，相对平滑些（相对稳定），不像SGD那样震荡的比较厉害。mini-batch gradient descent的一个缺点是增加了一个超参数 batch_sizebatch_size ，要去调这个超参数。 以上就是关于batch gradient descent、mini-batch gradient descent 和 stochastic gradient descent的内容。</p><h1 id="part-ii-深度学习中优化方法momentumnesterov-momentumadagradadadeltarmspropadam">Part II: 深度学习中优化方法——momentum、Nesterov Momentum、AdaGrad、Adadelta、RMSprop、Adam</h1><p>我们通常使用梯度下降来求解神经网络的参数，关于梯度下降前面一篇博客已经很详细的介绍了（几种梯度下降方法对比）。我们在梯度下降时，为了加快收敛速度，通常使用一些优化方法，比如：momentum、RMSprop和Adam等。这篇博客主要介绍：</p><ul><li>指数加权平均（Exponentially weighted average）</li><li>带偏差修正的指数加权平均（bias correction in exponentially weighted average）</li><li>Momentum</li><li>Nesterov Momentum</li><li>Adagrad</li><li>Adadelta</li><li>RMSprop</li><li>Adam</li></ul><p>在介绍这几种优化方法之前，必须先介绍下 <strong>指数加权平均（Exponentially weighted average）</strong> ，因为这个算法是接下来将要介绍的三个算法的重要组成部分。</p><h2 id="一-指数加权平均exponentially-weighted-average">一、 指数加权平均（Exponentially weighted average）</h2><p>指数加权平均是处理时间序列的常用工具，下面用一个例子来引入指数加权平均的概念。下图是一个180天的气温图（图片来自ng Coursera deep learning 课）：</p><img data-src="/2019/03/11/9c2ab4054ae2/8.jpg" title="ewa"><p>如果我们想找一条线去拟合这个数据，该怎么去做呢。我们知道某一天的气温其实和前几天（前一段时间）相关的，并不是一个独立的随机事件，比如夏天气温都会普遍偏高些，冬天气温普遍都会低一些。我们用<span class="math inline">\([θ1,θ2,...,θn]\)</span>表示第1，2，…，n天的气温，我们有： <img data-src="/2019/03/11/9c2ab4054ae2/9.jpg" title="temperature"></p><p>根据上面的公式我们能够画出这条线，如下图所示（图片来自ng deep learning课）：</p><img data-src="/2019/03/11/9c2ab4054ae2/10.jpg" title="fit"><p>下面来正式的看一下 指数加权平均（Exponentially weighted average） 的定义：</p><p><span class="math display">\[V_t=βV_(t−1)+(1−β)θ_t\]</span></p><p>可以认为 <span class="math inline">\(V_t\)</span> 近似是 <span class="math inline">\(\frac{1}{1−β}\)</span> 天的平均气温，所以上面公式设置了 <span class="math inline">\(β=0.9\)</span> ，当 <span class="math inline">\(t&gt;10\)</span> 时，则可以认为 <span class="math inline">\(V_t\)</span> 近似是它前10天的平均气温。 比如，按照上面的公式，我们计算 <span class="math inline">\(V_{100}\)</span> ，</p><p><span class="math display">\[ \begin{aligned}V_{100}&amp;=0.1θ_{100}+0.9V_{99} \\ &amp;=0.1θ_{100}+0.9(0.9V_{98}+0.1θ_{99}) \\ &amp;=0.1θ_{100}+0.9∗0.1θ_{99}+0.9^2V_{98} \\ &amp;=0.1θ_{100}+0.9∗0.1θ_{99}+0.9^2(0.9V_{97}+0.1θ_{98}) \\ &amp;=0.1θ_{100}+0.9∗0.1θ_{99}+0.9^2∗0.1θ_{98}+0.9^3V_{97} \\ &amp;=0.1θ_{100}+0.9∗0.1θ_{99}+0.9^2∗0.1θ_{98}+0.9^3∗0.1θ_{97}+0.9^4V_{96} \\ &amp;=.... \\ &amp;=0.1θ_{100}+0.9∗0.1θ_{99}+0.9^2∗0.1θ_{98}+0.9^3∗0.1θ_{97}+...+0.9^{99}∗0.1θ_1+0.9^{100}V_0 \end{aligned} \]</span></p><p>从上面的公式能够看出，实际上是个指数衰减函数。<span class="math inline">\(0.9^{10} ≈ 0.35 ≈ \frac{1}{e}\)</span>，所以这个就解释了上面的 <span class="math inline">\(\frac{1}{1−β}\)</span>。</p><p>（ps：关于这一点为什么要用0.35及 <span class="math inline">\(\frac{1}{e}\)</span>，我是不太明白的，搜了下资料也没找到更好的资料，希望路过的有知道的大神，在下面评论交流。）</p><p>下面再看下 β 取不同值时，曲线的拟合情况（图片来自ng deep learning课）：</p><img data-src="/2019/03/11/9c2ab4054ae2/11.jpg" title="fit"><p>从上图能够看出：</p><p>当 β 较大时（β=0.98 相当于每一点前50天的平均气温），曲线波动相对较小更加平滑（绿色曲线），因为对很多天的气温做了平均处理，正因为如此，曲线还会右移。 同理，当 β 较小时（β=0.5 相当于每一点前2天的平均气温），曲线波动相对激烈，但是它可以更快的适应温度的变化。 下面直接看实现指数加权平均（Exponentially weighted average）的伪代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">V0 = 0</span><br><span class="line">repeat</span><br><span class="line">&#123;</span><br><span class="line">    get next theta_t</span><br><span class="line">    V_theta = beta * V_theta + (1 - beta)* theta_t</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于指数加权平均的优缺点： 当 β=0.9β=0.9 时，我们可以近似的认为当前的数值是过去10天的平均值，但是显然如果我们直接计算过去10天的平均值，要比用指数加权平均来的更加准确。但是如果直接计算过去10天的平均值，我们要存储过去10天的数值，而加权平均只要存储Vt−1Vt−1一个数值即可，而且只需要一行代码即可，所以在机器学习中用的很多。</p><h2 id="二带偏差修正的指数加权平均bias-correction-in-exponentially-weighted-average">二、带偏差修正的指数加权平均（bias correction in exponentially weighted average）</h2><p>上一节中介绍了指数加权平均，用的公式是： <span class="math display">\[V_t=βV_{t−1{+(1−β)θ_t\]</span></p><p>我们想得到下图中的“绿线”，实际上我们得到的是下图中的“紫线”。对比这两条线，能够发现在“紫线”的起点相比“绿线”非常的低，也就是说 指数加权平均 不能很好地拟合前几天的数据，因此需要 偏差修正，解决办法就是，再令<span class="math inline">\(V_t=\frac{V_t}{1-\beta^t}\)</span>. 因此， 带偏差修正的指数加权平均（bias correction in exponentially weighted average） 公式为：</p><p><span class="math display">\[V_t=βV_{t−1}+(1−β)θ_t\]</span> <span class="math display">\[V_t=\frac{V_t}{1-\beta^t}\]</span></p><p>当 <span class="math inline">\(t\to\infty\)</span> 时，<span class="math inline">\(\beta_t\to0\)</span>，这样在后期 Vt 就和 没有修正的指数加权平均一样了.</p><blockquote><p>在机器学习中，多数的指数加权平均运算并不会使用偏差修正。因为大多数人更愿意在初始阶段，用一个捎带偏差的值进行运算。不过，如果在初试阶段就开始考虑偏差，指数加权移动均值仍处于预热阶段，偏差修正可以做出更好的估计。</p></blockquote><p>介绍完上面的准备知识，下面正式进入正题。</p><h2 id="三动量momentum">三、动量（momentum）</h2><p>        我们使用SGD（stochastic mini-batch gradient descent，深度学习中一般称之为SGD）训练参数时，有时候会下降的非常慢，并且可能会陷入到局部最小值中，如下图所示（图片来自李宏毅《一天搞懂深度学习》）。</p><img data-src="/2019/03/11/9c2ab4054ae2/12.jpg"><p>下面给出动量（momentum）的公式：</p><p>动量的引入就是为了加快学习过程，特别是对于高曲率、小但一致的梯度，或者噪声比较大的梯度能够很好的加快学习过程。动量的主要思想是积累了之前梯度指数级衰减的移动平均（前面的指数加权平均），下面用一个图来对比下，SGD和动量的区别：</p><img data-src="/2019/03/11/9c2ab4054ae2/13.jpg"><p>区别： SGD每次都会在当前位置上沿着负梯度方向更新（下降，沿着正梯度则为上升），并不考虑之前的方向梯度大小等等。而动量（moment）通过引入一个新的变量 vv 去积累之前的梯度（通过指数衰减平均得到），得到加速学习过程的目的。</p><blockquote><p>最直观的理解就是，若当前的梯度方向与累积的历史梯度方向一致，则当前的梯度会被加强，从而这一步下降的幅度更大。若当前的梯度方向与累积的梯度方向不一致，则会减弱当前下降的梯度幅度。</p></blockquote><p>用一个图来形象的说明下上面这段话（图片来自李宏毅《一天搞懂深度学习》）：</p><img data-src="/2019/03/11/9c2ab4054ae2/14.jpg"><p>下面给出动量（momentum）的公式：</p><p><span class="math display">\[\begin{aligned} &amp;V_{dW}=βV_{dW}+(1−β)dW \\ &amp;V_{db}=βV_{db}+(1−β)db \\ &amp;W=W-\alpha V_{dW}, b=b-\alpha V_{db}\end{aligned}\]</span></p><p>β的值越大，则之前的梯度对现在的方向影响越大。β一般取值为0.5，0.9，0.99。ng推荐取值0.9。这个公式是ng的在Coursera课上给出的。</p><p>关于这个公式目前主要有<strong>两种形式</strong>，*一种是原论文里的公式：（原论文：<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~hinton/absps/momentum.pdf">On the importance of initialization and momentum in deep learning</a>）</p><img data-src="/2019/03/11/9c2ab4054ae2/15.jpg"><p>使用这个公式的有： 1、 goodfellow和bengio的《deep learning》书：（<a target="_blank" rel="noopener" href="http://www.deeplearningbook.org/contents/optimization.html">8.3.2节 momentum</a>） <img data-src="/2019/03/11/9c2ab4054ae2/16.jpg"> 2、 CS231N课（链接：http://cs231n.github.io/neural-networks-3/）： 3、 keras： 4、Intel的chainer框架</p><p>*第二种是类似ng给的：</p><p>1、论文《An overview of gradient descent optimization algorithms》： 2、TensorFlow源码里的版本（链接：<a href="tensorflow/tensorflow/python/training/momentum.py"></a>）： 3、pytorch源码里的版本（链接：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/master/_modules/torch/optim/sgd.html#SGD"></a> ）：</p><p>4、当时修hinton的nn课时，hinton给出的版本与TensorFlow这个是一样的，链接见我的博客：</p><p>这个版本是和ng在课上给出的版本是一致的，只不过会影响下learning_rate的取值。 综上，应该是哪个版本都可以。大家根据自己的喜好使用。</p><p>由于我个人更喜欢ng/TensorFlow/pytorch那个形式，下面无论是代码还是伪代码我都会统一用ng的版本，下面给出momentum的伪代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">initialize VdW = 0, vdb = 0 //VdW维度与dW一致，Vdb维度与db一致</span><br><span class="line">on iteration t:</span><br><span class="line">    compute dW,db on current mini-batch</span><br><span class="line">    VdW = beta*VdW + (1-beta)*dW</span><br><span class="line">    Vdb = beta*Vdb + (1-beta)*db</span><br><span class="line">    W = W - learning_rate * VdW</span><br><span class="line">    b = b - learning_rate * Vdb</span><br></pre></td></tr></table></figure><p>具体的代码实现为：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_velocity</span>(<span class="params">parameters</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Initializes the velocity as a python dictionary with:</span></span><br><span class="line"><span class="string">                - keys: &quot;dW1&quot;, &quot;db1&quot;, ..., &quot;dWL&quot;, &quot;dbL&quot;</span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters[&#x27;W&#x27; + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters[&#x27;b&#x27; + str(l)] = bl</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity.</span></span><br><span class="line"><span class="string">                    v[&#x27;dW&#x27; + str(l)] = velocity of dWl</span></span><br><span class="line"><span class="string">                    v[&#x27;db&#x27; + str(l)] = velocity of dbl</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    <span class="comment"># Initialize velocity</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        v[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)].shape)</span><br><span class="line">        v[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)].shape)</span><br><span class="line">    <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters_with_momentum</span>(<span class="params">parameters, grads, v, beta, learning_rate</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Update parameters using Momentum</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters[&#x27;W&#x27; + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters[&#x27;b&#x27; + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads[&#x27;dW&#x27; + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads[&#x27;db&#x27; + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></span><br><span class="line"><span class="string">                    v[&#x27;dW&#x27; + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v[&#x27;db&#x27; + str(l)] = ...</span></span><br><span class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters</span></span><br><span class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    <span class="comment"># Momentum update for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        <span class="comment"># compute velocities</span></span><br><span class="line">        v[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = beta * v[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">&#x27;dW&#x27;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = beta * v[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">&#x27;db&#x27;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)]</span><br><span class="line">        <span class="comment"># update parameters</span></span><br><span class="line">        parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] - learning_rate * v[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] - learning_rate * v[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure><h2 id="四nesterov-momentum">四、Nesterov Momentum</h2><p>Nesterov Momentum是对Momentum的改进，可以理解为nesterov动量在标准动量方法中添加了一个校正因子。用一张图来形象的对比下momentum和nesterov momentum的区别（图片来自：http://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf）：</p><img data-src="/2019/03/11/9c2ab4054ae2/17.jpg"><p>公式：</p><p><span class="math display">\[\begin{aligned}\\ v_{t+1}&amp;=\mu v_t - \varepsilon \nabla f(\theta_t + \mu v_t) \\ \theta_{t+1} &amp;= \theta_{t} + v_{t+1}\end{aligned}\]</span></p><blockquote><p>但是，我们仔细观察这个算法，你会发现一个很大的缺点，这个算法会导致运行速度巨慢无比，因为这个算法每次都要计算<span class="math inline">\(\nabla_{\theta} \sum_i{L(f({x}^{i}; \theta + \alpha v), y^{i})}\)</span>，这个相当于又要把fp、bp走一遍。这样就导致这个算法的运行速度比momentum要慢两倍，因此在实际实现过程中几乎没人直接用这个算法，而都是采用了变形版本。</p></blockquote><p>变形版本：把上面的公式第一步代入第二步可以得到相同的公式：<span class="math inline">\(w = w + {\beta}^2 V - (1+\beta)\alpha * \nabla\)</span>。我这里直接使用keras风格公式了，其中 β 是动量参数，α 是学习率。</p><p><span class="math display">\[\begin{aligned} V_{\mathrm{d}W} &amp;= \beta V_{\mathrm{d}W} - \alpha \mathrm{d}W \\ V_{\mathrm{d}b} &amp;= \beta V_{\mathrm{d}b} - \alpha \mathrm{d}b \\ W &amp;= W + \beta V \mathrm{d}W - \alpha \mathrm{d}W \\ b &amp;= b + \beta V \mathrm{d}b - \alpha \mathrm{d}b \\\end{aligned}\]</span></p><blockquote><p>这样写的高明之处在于，我们没必要去计算 <span class="math inline">\(∇_θ\)</span> 了，直接利用当前已求得的 <span class="math inline">\(\mathrm{d}θ\)</span> 去更新参数即可。这样就节省了一半的时间。</p></blockquote><h2 id="五adagradadaptive-gradient">五、AdaGrad（Adaptive Gradient）</h2><p>通常，我们在每一次更新参数时，对于所有的参数使用相同的学习率。而AdaGrad算法的思想是：每一次更新参数时（一次迭代），不同的参数使用不同的学习率。AdaGrad 的公式为：</p><img data-src="/2019/03/11/9c2ab4054ae2/18.jpg"><blockquote><p>关于AdaGrad，goodfellow和bengio的《deep learning》书中对此的描述是：在凸优化中，AdaGrad算法具有一些令人满意的理论性质。但是，在实际使用中已经发现，对于训练深度神经网络模型而言，从训练开始时累积梯度平方会导致学习率过早过量的减少。AdaGrad算法在某些深度学习模型上效果不错，但不是全部。</p></blockquote><h2 id="六adadelta">六、Adadelta</h2><p>Adadelta是对Adagrad的改进，主要是为了克服Adagrad的两个缺点（摘自Adadelta论文《AdaDelta: An Adaptive Learning Rate Method》）：</p><ul><li>the continual decay of learning rates throughout training</li><li>the need for a manually selected global learning rate</li></ul><p>为了解决第一个问题，Adadelta只累积过去 w 窗口大小的梯度，其实就是利用前面讲过的指数加权平均</p><p>为了解决第二个问题，Adadelta最终的公式不需要学习率 α。Adadelta的具体算法如下所示（来自论文《AdaDelta: An Adaptive Learning Rate Method》）</p><h2 id="七rmsproproot-mean-square-prop">七、RMSprop（root mean square prop）</h2><p>RMSprop是hinton老爷子在Coursera的《Neural Networks for Machine Learning》lecture6中提出的，这个方法并没有写成论文发表（不由的感叹老爷子的强大。。以前在Coursera上修过这门课，个人感觉不算简单）。同样的，RMSprop也是对Adagrad的扩展，以在非凸的情况下效果更好。和Adadelta一样，RMSprop使用指数加权平均（指数衰减平均）只保留过去给定窗口大小的梯度，使其能够在找到凸碗状结构后快速收敛。直接来看下RMSprop的算法（来自lan goodfellow 《deep learning》）</p><blockquote><p>在实际使用过程中，RMSprop已被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习人员经常采用的优化算法之一。keras文档中关于RMSprop写到：This optimizer is usually a good choice for recurrent neural networks.</p></blockquote><h2 id="八adamadaptive-moment-estimation">八、Adam（Adaptive Moment Estimation）</h2><p>Adam实际上是把momentum和RMSprop结合起来的一种算法，算法流程是（摘自adam论文）：</p><img data-src="/2019/03/11/9c2ab4054ae2/19.jpg"><blockquote><p>实践表明，Adam算法在很多种不同的神经网络结构中都是非常有效的。</p></blockquote><h2 id="八该如何选择优化算法">八、该如何选择优化算法</h2><p>介绍了这么多算法，那么我们到底该选择哪种算法呢？目前还没有一个共识，schaul et al 在大量学习任务上比较了许多优化算法，结果表明，RMSprop，Adadelta和Adam表现的相当鲁棒，不分伯仲。Kingma et al表明带偏差修正的Adam算法稍微好于RMSprop。总之，Adam算法是一个相当好的选择，通常会得到比较好的效果。</p><p>下面是论文《An overview of gradient descent optimization algorithms》对各种优化算法的总结：</p><blockquote><p>In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numerator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [10] show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice</p></blockquote><hr><p>作者：天泽28 来源：CSDN 原文： - https://blog.csdn.net/u012328159/article/details/80252012 - https://blog.csdn.net/u012328159/article/details/80311892 版权声明：本文为博主原创文章，转载请附上博文链接！</p></div><footer class="post-footer"><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>leefige</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://leefige.github.io/2019/03/11/9c2ab4054ae2/" title="【转载】几种梯度下降方法和优化方法对比">https://leefige.github.io/2019/03/11/9c2ab4054ae2/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="post-tags"><a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag"># 梯度下降</a> <a href="/tags/%E4%BC%98%E5%8C%96/" rel="tag"># 优化</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2019/03/05/15bef8e28185/" rel="prev" title="基于Nginx配置Web视频流媒体服务器"><i class="fa fa-angle-left"></i> 基于Nginx配置Web视频流媒体服务器</a></div><div class="post-nav-item"><a href="/2019/03/11/2a4cfaea7761/" rel="next" title="KaTex中的多行等式">KaTex中的多行等式 <i class="fa fa-angle-right"></i></a></div></div></footer></article></div><div class="comments" id="waline"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">leefige</span></div><div class="wordcount"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i> </span><span title="站点总字数">85k</span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">1:43</span></span></div><div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动</div></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><div class="sidebar-dimmer"></div><div class="back-to-top" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up fa-lg"></i> <span>0%</span></div><div class="reading-progress-bar"></div><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script><script src="/js/third-party/search/local-search.js"></script><script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"6qI1g7GDwX4vCFoxeoGwBFvi-MdYXbMMI","app_key":"HfMoYPgocV77zPAY1FlTy39w","server_url":"https://6qi1g7gd.api.lncldglobal.com","security":false}</script><script src="/js/third-party/statistics/lean-analytics.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/js/third-party/math/mathjax.js"></script><script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://blog-service-ruby.vercel.app","cssUrl":"https://unpkg.com/@waline/client@v3/dist/waline.css","commentCount":true,"pageview":false,"libUrl":"https://unpkg.com/@waline/client@v3/dist/waline.umd.js","locale":{"placeholder":"Please leave your comments here, and your contact above :-)"},"emoji":["https://unpkg.com/@waline/emojis@1.1.0/alus"],"el":"#waline","comment":true,"path":"/2019/03/11/9c2ab4054ae2/"}</script><link rel="stylesheet" href="https://unpkg.com/@waline/client@v3/dist/waline.css"><script>document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":100,"height":200},"mobile":{"show":false},"react":{"opacityDefault":1,"opacityOnHover":1},"log":false,"tagMode":false});</script></body></html>